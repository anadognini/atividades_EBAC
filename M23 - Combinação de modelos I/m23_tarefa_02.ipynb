{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 1. **Monte um passo a passo para o algoritmo RF:**\n",
        "O Random Forest possui três etapas:\n",
        "\n",
        "Passo 1. Bootstrap + Feature Selection: o primeiro passo é similar ao bootstrap do Bagging (ou seja, é feita uma amostragem com reposição a partir da qual são criados subconjuntos de um DataFrame inicial), porém com seleção de colunas. A seleção do número de colunas é feita conforme um cálculo, que varia de acordo com a tarefa (regressão ou classificação).\n",
        "\n",
        "Passo 2. Base learners: assim como no Bagging, a partir de cada subconjunto criado é treinado um modelo em paralelo. Porém, aqui no Random Forest, o modelo treinado é o modelo de árvore de decisão.\n",
        "\n",
        "Passo 3. O resultado final é calculado através de um \"soft voting\" ou \"sabedoria das multidões\". Nesse processo, a classe mais frequente de resultados é a classe final."
      ],
      "metadata": {
        "id": "9r-NSo-C9LQe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. **Explique com suas palavras o Random forest:**\n",
        "Random Forest é um método de aprendizado por conjunto construído em cima do Bagging, uma espécie de extensão. Foi criado em 1996 por Leo Breiman e Adele Cutler."
      ],
      "metadata": {
        "id": "zrNKorEF-TkU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3. Qual a diferença entre Bagging e Random Forest?\n",
        "A principal diferença entre o Baggine e o RF é a feature selection durante o processo de reamostragem. Essa aleatoriedade de funcionalidades gera um subconjunto aleatório de funcionalidades, o que gera baixa correlação entre as árvores de decisão. Enquanto as árvores de decisão consideram todas as possíveis divisões de funcionalidades, as florestas aleatórias selecionam apenas um subconjunto dessas funcionalidades."
      ],
      "metadata": {
        "id": "UsQP2HVB-8ko"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. (Opcional) Implementar em python o Random Forest"
      ],
      "metadata": {
        "id": "wPQK4_gm_jsL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13QeOuc43PXK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Literal, List, Tuple\n",
        "from sklearn.base import clone\n",
        "from sklearn.metrics import accuracy_score, root_mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from joblib import Parallel, delayed # Para paralelização\n",
        "\n",
        "TaskType = Literal[\"classification\", \"regression\"]\n",
        "VoteType = Literal[\"hard\", \"soft\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class RandomForestManual:\n",
        "    base_estimator: object\n",
        "    n_estimators: int = 200\n",
        "    task: TaskType = \"classification\"\n",
        "    voting: VoteType = \"soft\"  # Adicionado: Suporte a Soft Voting\n",
        "    m_features: Optional[int] = None\n",
        "    bootstrap: bool = True\n",
        "    n_jobs: int = -1           # Adicionado: Controle de paralelismo (-1 usa todos os cores)\n",
        "    random_state: int = 42\n",
        "\n",
        "    models_: Optional[List[object]] = None\n",
        "    oob_indices_: Optional[List[np.ndarray]] = None\n",
        "    feature_subsets_: Optional[List[List[str]]] = None\n",
        "    oob_score_: Optional[float] = None\n",
        "    classes_: Optional[np.ndarray] = None\n",
        "\n",
        "    def _rng(self):\n",
        "        return np.random.default_rng(self.random_state)\n",
        "\n",
        "    def _bootstrap_indices(self, n: int, rng: np.random.Generator):\n",
        "        if not self.bootstrap:\n",
        "            return np.arange(n), np.array([], dtype=int)\n",
        "        inbag = rng.integers(0, n, size=n)\n",
        "        oob = np.setdiff1d(np.arange(n), np.unique(inbag))\n",
        "        return inbag, oob\n",
        "\n",
        "    def _choose_features(self, X: pd.DataFrame, rng: np.random.Generator):\n",
        "        p = X.shape[1]\n",
        "        if self.m_features is None:\n",
        "            m = int(np.sqrt(p)) if self.task == \"classification\" else max(1, int(p / 3))\n",
        "        else:\n",
        "            m = self.m_features\n",
        "        m = max(1, min(m, p))\n",
        "        return list(rng.choice(X.columns, size=m, replace=False))\n",
        "\n",
        "    def _train_single_tree(self, X, y, seed):\n",
        "      rng_b = np.random.default_rng(seed)\n",
        "      inbag_idx, oob_idx = self._bootstrap_indices(X.shape[0], rng_b)\n",
        "      feat_subset = self._choose_features(X, rng_b)\n",
        "\n",
        "      model = clone(self.base_estimator)\n",
        "\n",
        "      # Ajuste para evitar o InvalidParameterError\n",
        "      if hasattr(model, \"max_features\"):\n",
        "          if self.task == \"classification\":\n",
        "              model.max_features = \"sqrt\"\n",
        "          else:\n",
        "              # Para regressão, calculamos 1/3 das colunas do subconjunto atual\n",
        "              model.max_features = max(1, int(len(feat_subset) / 3))\n",
        "\n",
        "      if hasattr(model, \"random_state\"):\n",
        "          model.random_state = seed\n",
        "\n",
        "      model.fit(X.iloc[inbag_idx][feat_subset], y.iloc[inbag_idx])\n",
        "      return model, oob_idx, feat_subset\n",
        "\n",
        "    def fit(self, X: pd.DataFrame, y: pd.Series):\n",
        "        if isinstance(y, pd.DataFrame): y = y.iloc[:, 0]\n",
        "        rng_master = self._rng()\n",
        "\n",
        "        if self.task == \"classification\":\n",
        "            self.classes_ = np.sort(y.unique())\n",
        "\n",
        "        seeds = rng_master.integers(0, 1_000_000, size=self.n_estimators)\n",
        "\n",
        "        results = Parallel(n_jobs=self.n_jobs)(\n",
        "            delayed(self._train_single_tree)(X, y, seed) for seed in seeds\n",
        "        )\n",
        "\n",
        "        self.models_, self.oob_indices_, self.feature_subsets_ = zip(*results)\n",
        "        self.oob_score_ = self._compute_oob_score(X, y)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
        "        if self.task == \"regression\":\n",
        "            preds = np.array([m.predict(X[f]) for m, f in zip(self.models_, self.feature_subsets_)])\n",
        "            return preds.mean(axis=0)\n",
        "\n",
        "        if self.voting == \"soft\":\n",
        "            return self._predict_soft(X)\n",
        "\n",
        "        preds = np.array([m.predict(X[f]) for m, f in zip(self.models_, self.feature_subsets_)])\n",
        "        return np.array([pd.Series(preds[:, i]).mode()[0] for i in range(preds.shape[1])])\n",
        "\n",
        "    def _predict_soft(self, X: pd.DataFrame) -> np.ndarray:\n",
        "        probas_list = []\n",
        "        for model, feats in zip(self.models_, self.feature_subsets_):\n",
        "            p = model.predict_proba(X[feats])\n",
        "\n",
        "            if not np.array_equal(model.classes_, self.classes_):\n",
        "                aligned = np.zeros((X.shape[0], len(self.classes_)))\n",
        "                for idx, cls in enumerate(model.classes_):\n",
        "                    loc = np.where(self.classes_ == cls)[0][0]\n",
        "                    aligned[:, loc] = p[:, idx]\n",
        "                probas_list.append(aligned)\n",
        "            else:\n",
        "                probas_list.append(p)\n",
        "\n",
        "        avg_proba = np.mean(probas_list, axis=0)\n",
        "        return self.classes_[np.argmax(avg_proba, axis=1)]\n",
        "\n",
        "    def score(self, X: pd.DataFrame, y: pd.Series) -> float:\n",
        "        if isinstance(y, pd.DataFrame): y = y.iloc[:, 0]\n",
        "        y_pred = self.predict(X)\n",
        "        return accuracy_score(y, y_pred) if self.task == \"classification\" else root_mean_squared_error(y, y_pred)\n",
        "\n",
        "    def _compute_oob_score(self, X_train: pd.DataFrame, y_train: pd.Series):\n",
        "        n = X_train.shape[0]\n",
        "        oob_preds = [[] for _ in range(n)]\n",
        "\n",
        "        for model, feats, oob_idx in zip(self.models_, self.feature_subsets_, self.oob_indices_):\n",
        "            if len(oob_idx) == 0: continue\n",
        "\n",
        "            preds = model.predict(X_train.iloc[oob_idx][feats])\n",
        "            for i, p in zip(oob_idx, preds):\n",
        "                oob_preds[i].append(p)\n",
        "\n",
        "        y_true, y_pred_final = [], []\n",
        "        for i in range(n):\n",
        "            if not oob_preds[i]: continue\n",
        "            y_true.append(y_train.iloc[i])\n",
        "            if self.task == \"classification\":\n",
        "                y_pred_final.append(pd.Series(oob_preds[i]).mode()[0])\n",
        "            else:\n",
        "                y_pred_final.append(np.mean(oob_preds[i]))\n",
        "\n",
        "        if not y_true: return None\n",
        "        return accuracy_score(y_true, y_pred_final) if self.task == \"classification\" else root_mean_squared_error(y_true, y_pred_final)"
      ],
      "metadata": {
        "id": "uWdJZYDp3fQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Teste 1 - Classificação"
      ],
      "metadata": {
        "id": "6Y7ELwA__ov1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "\n",
        "wine = load_wine(as_frame=True)\n",
        "X_w, y_w = wine.data, wine.target\n",
        "X_train_w, X_test_w, y_train_w, y_test_w = train_test_split(X_w, y_w, test_size=0.3, stratify=y_w, random_state=42)\n",
        "\n",
        "rf_fixed = RandomForestManual(\n",
        "    base_estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=300,\n",
        "    task=\"classification\",\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf_fixed.fit(X_train_w, y_train_w)\n",
        "\n",
        "print(\"OOB Accuracy:\", rf_fixed.oob_score_)\n",
        "print(\"Test Accuracy:\", rf_fixed.score(X_test_w, y_test_w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ltpki71O4CqS",
        "outputId": "76160adb-497d-415c-f9f1-43e1a452e227"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOB Accuracy: 0.9758064516129032\n",
            "Test Accuracy: 0.9814814814814815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Teste 2 - Regressão"
      ],
      "metadata": {
        "id": "77T3jPzW_sXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "data = fetch_california_housing(as_frame=True)\n",
        "X, y = data.frame.drop(columns=['MedHouseVal']), data.frame['MedHouseVal']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "rf_fixed = RandomForestManual(\n",
        "    base_estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=300,\n",
        "    task=\"regression\",\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf_fixed.fit(X_train, y_train)\n",
        "\n",
        "print(\"OOB RMSE:\", rf_fixed.oob_score_)\n",
        "print(\"Test RMSE:\", rf_fixed.score(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2WRgk2X4E37",
        "outputId": "dc01b680-153d-44da-f2dc-512bd55323a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOB RMSE: 0.7921029201780437\n",
            "Test RMSE: 0.7811466085835139\n"
          ]
        }
      ]
    }
  ]
}