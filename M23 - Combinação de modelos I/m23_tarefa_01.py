# -*- coding: utf-8 -*-
"""m23_tarefa_01.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WRvwH9sTfuYFQMyzeBtfAw0ueu3ObYDr

### 1. **Monte um passo a passo para o Bagging:**
O Bagging possui três etapas:

Passo 1. Bootstrapping ou amostragem com reposição: é feita uma reamostragem de um conjunto de dados, gerando subconjuntos diferentes. Isso é feito por meio da seleção aleatória dos dados com reposição. A cada seleção de um dado do conjunto de treinamento, é possível selecionar a mesma instância mais de uma vez. Por isso, um dado ou instância pode aparecer repetidamente em uma mesma amostra.

Passo 2. Base learners: essas amostras são treinadas de forma independente, utilizando modelos que podem ser uma árvore de decisão ou uma regressão ou outro modelo simples. Esses modelos são usados para classificar.

Passo 3. Agregação: dependendo da tarefa (regressão ou classificação), os resultados são estimados de maneiras diferentes:
- Classificação: a classe com a maioria de votos é aceita. É feita uma tabela de frequência com as classes (resultados) obtidos por cada modelo e suas frequências e que possuir maior frequência, é escolhida.
- Regressão: é feita uma média de todos os resultados previstos.

### 2. **Explique com suas palavras o Bagging:**
Bagging é um método de aprendizado por conjunto muito comum e muito pedido em entrevistas de emprego por recrutadores. Em essência, ele treina vários modelos independentes em diferentes amostras bootstrap dos dados e combina suas previsões para obter um modelo mais estável e preciso.

### **3. (Opcional) Implementar em python o código do Bagging**
"""

# Preparação (dados e imports)

import numpy as np
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
import pandas as pd

# Bootstrap (amostragem com reposição)
# Ideia: criar uma amostra bootstrap do treino, com o mesmo tamanho, pegando
# índices aleatórios com reposição.

def bootstrap_sample(X, y, random_state=42):
    rng = np.random.default_rng(random_state)
    idx = rng.integers(0, X.shape[0], size=X.shape[0])
    return X.iloc[idx], y.iloc[idx]

# Modelagem (treinar vários modelos)
# Vamos treinar x modelos base (ex: árvores)

def train_bagging_models(X_train, y_train, B=50, random_state=42):
    models = []
    rng = np.random.default_rng(random_state)

    for b in range(B):
        # bootstrap
        idx_seed = rng.integers(0, 1_000_000)
        X_b, y_b = bootstrap_sample(X_train, y_train, random_state=idx_seed)

        # modelo base
        model = DecisionTreeClassifier(
            max_depth=None,
            random_state=idx_seed
        )
        model.fit(X_b, y_b)
        models.append(model)

    return models

# Agregação (votação majoritária)
# Para classificação, o padrão é:
# - cada modelo dá uma predição;
# - fazemos votação majoritária por amostra

def bagging_predict(models, X):
    preds = np.array([m.predict(X) for m in models])  # shape: (B, n_amostras)

    # votação majoritária
    y_pred = []
    for i in range(preds.shape[1]):
        values, counts = np.unique(preds[:, i], return_counts=True)
        y_pred.append(values[np.argmax(counts)])

    return np.array(y_pred)

# Teste com dataset glass: https://www.kaggle.com/datasets/uciml/glass

df = pd.read_csv('glass.csv')

X = df.iloc[:, 0:-1]
y = df['Type']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

models = train_bagging_models(X_train, y_train, B=100, random_state=42)

y_pred = bagging_predict(models, X_test)

acc = accuracy_score(y_test, y_pred)
print(f'Acurácia Bagging (manual): {acc:.4f}')