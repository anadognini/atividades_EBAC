# -*- coding: utf-8 -*-
"""m24_tarefa_01.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bl4eGOyEKtKRXHzLXQXWTKv8GS6gjrZS

### 1. Cite 5 diferenças entre o Random Forest e o AdaBoost

#### Diferenças entre Random Forest e AdaBoost

| Característica              | Random Forest                       | AdaBoost                               |
|:----------------------------|:------------------------------------|:---------------------------------------|
| **Tipo de Base Learner**    | Floresta de árvores completas.      | Floresta de stumps (árvores rasas).    |
| **Paralelismo**             | Árvores trabalham paralela e independentemente. | Uma árvore influencia na árvore seguinte (sequencial). |
| **Ponderação das Respostas** | Resposta das árvores têm o mesmo peso. | Respostas das árvores têm pesos diferentes. |
| **Método de Ensemble**      | Bagging (Bootstrap Aggregation).    | Boosting (cada modelo tenta corrigir erros do anterior). |
| **Sensibilidade a Outliers**| Menos sensível a outliers.          | Mais sensível a outliers.              |

### 2. Acesse o link [Scikit-learn - Adaboost](https://scikit-learn.org/stable/modules/ensemble.html), leia a explicação (traduza se for preciso) e crie um Jupyter notebook contendo o exemplo do AdaBoost.
"""

# The following example shows how to fit an AdaBoost classifier with 100 weak
# learners

from sklearn.model_selection import cross_val_score
from sklearn.datasets import load_iris
from sklearn.ensemble import AdaBoostClassifier

X, y = load_iris(return_X_y=True)
clf = AdaBoostClassifier(n_estimators=100)
scores = cross_val_score(clf, X, y, cv=5)
scores.mean()

"""### 3. Cite 5 Hyperparametros importantes no AdaBoost.

1.  **`n_estimators`**: controla o número de estimadores (modelos fracos, geralmente árvores de decisão rasas) a serem sequencialmente ajustados. Um número maior pode levar a um melhor desempenho, mas também aumenta o risco de overfitting e o tempo de treinamento.
2.  **`learning_rate`**: peso aplicado à contribuição de cada estimador. Um `learning_rate` menor requer um `n_estimators` maior para atingir o mesmo desempenho, mas pode resultar em modelos mais robustos e evitar overfitting.
3.  **`estimator`** (anteriormente `base_estimator`): o objeto estimador fraco a partir do qual o ensemble é construído. Por padrão, é um `DecisionTreeClassifier` com `max_depth=1` (conhecido como stump). Você pode especificar outros estimadores, mas eles devem suportar ponderação de amostras.
4.  **`max_depth`** (do `estimator`): se o `estimator` for uma árvore de decisão, este hiperparâmetro controla a profundidade máxima da árvore. Para AdaBoost, é comum usar `max_depth=1` (stumps) para manter os estimadores fracos. Profundidades maiores podem introduzir complexidade e risco de overfitting.
5.  **`algorithm`**: define o algoritmo de boosting. As opções mais comuns são `'SAMME'` (usa predições de classe discretas) e `'SAMME.R'` (usa predições de probabilidade). `'SAMME.R'` geralmente tem melhor desempenho e é o padrão.

### 4. (Opcional) Utilize o GridSearch para encontrar os melhores hyperparametros para o conjunto de dados do exemplo (load_iris)
"""

from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier

# Define the base estimator (DecisionTreeClassifier with max_depth=1 is default for AdaBoost)
base_estimator = DecisionTreeClassifier(random_state=42)

# Create an AdaBoostClassifier instance
adaboost_clf = AdaBoostClassifier(estimator=base_estimator, random_state=42)

# Define the parameter grid to search
param_grid = {
    'n_estimators': [50, 100, 150, 200],
    'learning_rate': [0.01, 0.1, 0.5, 1.0],
    'estimator__max_depth': [1, 2, 3] # Explore max_depth for the base DecisionTreeClassifier
}

# Initialize GridSearchCV
grid_search = GridSearchCV(adaboost_clf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)

# Fit GridSearchCV to the data
grid_search.fit(X, y)

# Print the best parameters and best score
print(f"Melhores parâmetros: {grid_search.best_params_}")
print(f"Melhor score (acurácia): {grid_search.best_score_:.4f}")