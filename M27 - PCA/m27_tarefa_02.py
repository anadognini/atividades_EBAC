# -*- coding: utf-8 -*-
"""m27_tarefa_02.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JatceYsXlLH-yRGY6Xfgexyy8sB0k5V6

# Classificação de Atividade Humana com PCA

Vamos trabalhar com a base da demonstração feita em aula, mas vamos explorar um pouco melhor como é o desempenho da árvore variando o número de componentes principais.
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

from sklearn.tree import DecisionTreeClassifier

from sklearn.decomposition import PCA
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, ConfusionMatrixDisplay

filename_features = "/content/features.txt"
filename_labels = "/content/activity_labels.txt"

filename_subtrain = "/content/subject_train.txt"
filename_xtrain = "/content/X_train.txt"
filename_ytrain = "/content/y_train.txt"

filename_subtest = "/content/subject_test.txt"
ffilename_xtest = "/content/X_test.txt"
filename_ytest = "/content/y_test.txt"

# Rótulos, labels da atividade
features = pd.read_csv(filename_features, header=None, names=['nome_var'], sep="#")['nome_var']
labels = pd.read_csv(filename_labels, delim_whitespace=True, header=None, names=['cod_label', 'label'])

# Treino
subject_train = pd.read_csv(filename_subtrain, header=None, names=['subject_id'])['subject_id']
X_train = pd.read_csv(filename_xtrain, delim_whitespace=True, header=None, names=features.tolist())
y_train = pd.read_csv(filename_ytrain, header=None, names=['cod_label'])

# Teste
subject_test = pd.read_csv(filename_subtest, header=None, names=['subject_id'])['subject_id']
X_test = pd.read_csv(ffilename_xtest, delim_whitespace=True, header=None, names=features.tolist())
y_test = pd.read_csv(filename_ytest, header=None, names=['cod_label'])

"""## PCA com variáveis padronizadas

Reflexão sobre a escala das variáveis:

**Variáveis em métricas muito diferentes** podem interferir na análise de componentes principais. Lembra que variância é informação pra nós? Pois bem, tipicamente se há uma variável monetária como salário, vai ter uma ordem de variabilidade bem maior que número de filhos, tempo de emprego ou qualquer variável dummy. Assim, as variáveis de maior variância tendem a "dominar" a análise. Nesses casos é comum usar a padronização das variáveis.

Faça duas análises de componentes principais para a base do HAR - com e sem padronização e compare:

- A variância explicada por componente
- A variância explicada acumulada por componente
- A variância percentual por componente
- A variância percentual acumulada por componente
- Quantas componentes você escolheria, em cada caso para explicar 90% da variância?
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# def padroniza(s):
#     if s.std() > 0:
#         s = (s - s.mean())/s.std()
#     return s
# 
# X_train_pad = pd.DataFrame(X_train).apply(padroniza, axis=0)
# X_train_pad.head()

# Sem padronização
pca_raw = PCA().fit(X_train)

# Com padronização
pca_pad = PCA().fit(X_train_pad)

def get_pca_metrics(pca_model):
    variancia = pca_model.explained_variance_
    variancia_acumulada = np.cumsum(variancia)
    variancia_percentual = pca_model.explained_variance_ratio_
    variancia_perc_acumulada = np.cumsum(variancia_percentual)

    # Components needed for 90% variance
    n_90 = np.argmax(variancia_perc_acumulada >= 0.90) + 1

    return variancia, variancia_acumulada, variancia_percentual, variancia_perc_acumulada, n_90

res_raw = get_pca_metrics(pca_raw)
res_pad = get_pca_metrics(pca_pad)

print(f"Componentes para 90% de variância (Bruto): {res_raw[4]}")
print(f"Componentes para 90% de variância (Standardized): {res_pad[4]}")

fig, ax = plt.subplots(1, 2, figsize=(16, 6))

# Plot for Raw Data
ax[0].plot(res_raw[3], label='Variância Acumulada (Bruto')
ax[0].axhline(y=0.9, color='r', linestyle='--', label='90% Threshold')
ax[0].set_title(f'PCA Com Dados Brutos\n90% de Variância: {res_raw[4]} componentes')
ax[0].legend()

# Plot for Standardized Data
ax[1].plot(res_pad[3], label='Variância Acumulada (Standardized)', color='orange')
ax[1].axhline(y=0.9, color='r', linestyle='--', label='90% Threshold')
ax[1].set_title(f'PCA Com Dados Padronizados\n90% de Variância: {res_pad[4]} componentes')
ax[1].legend()

plt.show()

"""Embora o modelo padronizado precise de mais componentes para atginir o limite de 90%, ele geralmente é mais robusto. Em modelos de Machine Learning (como a `DecisionTreeClassifier`), usar os componentes padronizados costuma resultar em uma generalização melhor, pois o modelo aprende com base na variação real de cada sinal, e não apenas na magnitude bruta.

Nos dados sem padronização, as variáveis que possuem escalas maiores (ex: um sensor que marca -100 a 100) dominam completamente aquelas que marcam de -1 a 1. O PCA enxerga essa oscilação como a grande "informação principal". Ele consegue explicar 90% da base rapidamente apenas focando nessas poucas variáveis. O resultado é que você precisa de poucos componentes (34) porque a variância está concentrada em poucos lugares.

Quando você padroniza, você está dizendo ao algoritmo "Todas as variáveis agora têm o mesmo peso (Variância = 1)". O PCA perde aqueles "alvos fáceis" de alta variância. Agora, para explicar 90% da informação total, ele precisa olhar para quase todas as variáveis, pois a informaçãoe está espalhada uniformemente por todo o dataset. Como não há mais variáveis soberanas, o PCA precisa de mais eixos (63) para conseguir somar a mesma quantidade de explicação.

## Árvore com PCA

Faça duas uma árvore de decisão com 10 componentes principais - uma com base em dados padronizados e outra sem padronizar. Utilize o ```ccp_alpha=0.001```.

Compare a acurácia na base de treino e teste.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # PCA sem padronização
# pca_raw_10 = PCA(n_components=10).fit(X_train)
# X_train_pca_raw = pca_raw_10.transform(X_train)
# X_test_pca_raw = pca_raw_10.transform(X_test)

# Com padronização
pca_pad_10 = PCA(n_components=10).fit(X_train_pad)
X_train_pca_pad = pca_pad_10.transform(X_train_pad)
X_test_pad = pd.DataFrame(X_test).apply(padroniza, axis=0)
X_test_pca_pad = pca_pad_10.transform(X_test_pad)

def avaliar_arvore(X_tr, y_tr, X_te, y_te, titulo):
    clf = DecisionTreeClassifier(random_state=42, ccp_alpha=0.001)
    clf.fit(X_tr, y_tr)

    acc_train = accuracy_score(y_tr, clf.predict(X_tr))
    acc_test = accuracy_score(y_te, clf.predict(X_te))

    print(f"--- {titulo} ---")
    print(f"Acurácia Treino: {acc_train:.4f}")
    print(f"Acurácia Teste:  {acc_test:.4f}\n")

    return clf

clf_raw = avaliar_arvore(X_train_pca_raw, y_train, X_test_pca_raw, y_test, "PCA Sem Padronização (10 CP)")
clf_pad = avaliar_arvore(X_train_pca_pad, y_train, X_test_pca_pad, y_test, "PCA Com Padronização (10 CP)")

ConfusionMatrixDisplay.from_estimator(clf_pad, X_test_pca_pad, y_test, display_labels=labels['label'], xticks_rotation=45)

plt.title("Matriz de Confusão - Dados Padronizados")
plt.show()

"""**Por que o modelo sem padronização chegou a 82.4% de acurácia no teste, enquanto o padronizado ficou em 77.3%?**

- Densidade de informação: como falamos antes, a PCA sem padronização precisa de apenas 34 componentes para explicar 90% da variância. Isso significa que os primeiros 10 componentes já carregam uma parte grande da informação total do dataset.
- Diluição da padronização: quando você padroniza, você espalha a informação. Ao limitar o modelo a apenas 10 componentes, você está efetivamente jogando muito mais informação no cenário padronizado do que no cenário bruto.
  - No cenário bruto, os 10 CP são como os 10 capítulos principais de um livro.
  - No cenário padronizado, os 10 CP são como as primeiras 10 páginas de 60 capítulos.

Além disso, o modelo sem padronização não apenas performou melhor, como também parece ser generalizado um pouco melhor (menor queda de performance do treino para o teste). Isso indica que, para o problema do HAR, as variáveis de maior escala original são realmente as mais discriminantes para as atividades físicas.

Isso tudo fica ainda mais evidente na matriz de confusão:
- O modelo previu STANDING para 202 casos que eram, na verdade, SITTING.
- Da mesma forma, previu SITTING para 92 casos de STANDING.
-Isso provavelmente ocorreu porque, nestas duas atividades, o acelerômetro do celular registrou basicamente a mesma coisa: a aceleração da gravidade constante. Sem componentes suficientes para captar a inclinação sutil do aparelho ou mudanças mínimas de pressão, a PCA acabou misturando essas duas classes estáticas.
<br>

- WALKING_DOWNSTAIRS é classe a que mais sofreu: 70 casos foram confundidos com caminhada plana e 47 com subida de escadas.
- O padrão rítmico de Walking, Upstairs, Downstairs é muito similar. Com apenas 10 componentes, o modelo consegue entender que o usuário está andando, mas não tem resolução suficiente para distinguir o esforço vertical (subir/descer) da caminhada simples.
<br>

- A classe LAYING foi a mais fácil de classificar (489 acertos).
- Quando o usuário se deita, a orientação do sensor em relação à gravidade muda drasticamente (o eixo que antes era vertical pasas a ser horizontal). Essa é uma variação de sinal tão grande que mesmo o PCA "diluído" com padronização consegue capturar com facilidade.
"""