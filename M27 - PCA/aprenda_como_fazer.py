# -*- coding: utf-8 -*-
"""aprenda_como_fazer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N2NLclYWV-Nldeis5h5oQfmO0raBzWtU
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.tree import DecisionTreeClassifier

from sklearn.decomposition import PCA
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, ConfusionMatrixDisplay
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

filename_features = "/content/features.txt"
filename_labels = "/content/activity_labels.txt"

filename_subtrain = "/content/subject_train.txt"
filename_xtrain = "/content/X_train.txt"
filename_ytrain = "/content/y_train.txt"

filename_subtest = "/content/subject_test.txt"
ffilename_xtest = "/content/X_test.txt"
filename_ytest = "/content/y_test.txt"

# Rótulos, labels da atividade
features = pd.read_csv(filename_features, header=None, names=['nome_var'], sep="#")['nome_var']
labels = pd.read_csv(filename_labels, delim_whitespace=True, header=None, names=['cod_label', 'label'])

# Treino
subject_train = pd.read_csv(filename_subtrain, header=None, names=['subject_id'])['subject_id']
X_train = pd.read_csv(filename_xtrain, delim_whitespace=True, header=None, names=features.tolist())
y_train = pd.read_csv(filename_ytrain, header=None, names=['cod_label'])

# Teste
subject_test = pd.read_csv(filename_subtest, header=None, names=['subject_id'])['subject_id']
X_test = pd.read_csv(ffilename_xtest, delim_whitespace=True, header=None, names=features.tolist())
y_test = pd.read_csv(filename_ytest, header=None, names=['cod_label'])

X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# clf = DecisionTreeClassifier(random_state=1234).fit(X_train, y_train)
# caminho = DecisionTreeClassifier(random_state=2360873, min_samples_leaf=20).cost_complexity_pruning_path(X_train, y_train)
# ccp_alphas, impurities = caminho.ccp_alphas, caminho.impurities
# 
# ccp_alphas = np.unique(ccp_alphas[ccp_alphas>=0])
# 
# clfs = []
# 
# for ccp_alpha in ccp_alphas:
#     clf = DecisionTreeClassifier(random_state=2360873, ccp_alpha=ccp_alpha).fit(X_train, y_train)
#     clfs.append(clf)

train_scores = [clf.score(X_train, y_train) for clf in clfs]
valid_scores = [clf.score(X_valid, y_valid) for clf in clfs]

fig, ax = plt.subplots()
ax.set_xlabel("alpha")
ax.set_ylabel("Acurácia")
ax.set_title("Acurácia x alpha do conjunto de dados de treino e validação")
ax.plot(ccp_alphas, train_scores, marker='o', label="treino", drawstyle="steps-post")
ax.plot(ccp_alphas, valid_scores, marker='o', label="validação", drawstyle="steps-post")
ax.legend()
plt.show()

ind_melhor_arvore = len(valid_scores) - valid_scores[::-1].index(max(valid_scores)) - 1
melhor_arvore = clfs[ind_melhor_arvore]

acc_train = train_scores[ind_melhor_arvore]
acc_valid = valid_scores[ind_melhor_arvore]
acc_teste = melhor_arvore.score(X_test, y_test)

print(f'Acurácia da melhor árvore na base de treino:    {acc_train*100:.1f}')
print(f'Acurácia da melhor árvore na base de validação: {acc_teste*100:.1f}')
print(f'Acurácia da melhor árvore na base de teste:     {acc_teste*100:.1f}')

"""### Construindo o modelo com PCA"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# prcomp = PCA().fit(X_train)
# 
# pc_treino = prcomp.transform(X_train)
# pc_valida = prcomp.transform(X_valid)
# pc_teste  = prcomp.transform(X_test)
# 
# pc_treino.shape

n=60

colunas = ['cp' + str(x + 1) for x in list(range(n))] # Gera os nomes das componentes principais

pc_train = pd.DataFrame(pc_treino[:, :n], columns = colunas)
pc_valid = pd.DataFrame(pc_valida[:, :n], columns = colunas)
pc_test = pd.DataFrame(pc_teste[:, :n], columns = colunas)

pc_train.head()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# clf = DecisionTreeClassifier(random_state=1234).fit(pc_train, y_train)
# 
# caminho = DecisionTreeClassifier(random_state=2360873, min_samples_leaf=20).cost_complexity_pruning_path(pc_train, y_train)
# ccp_alphas, impurities = caminho.ccp_alphas, caminho.impurities
# 
# ccp_alphas = np.unique(ccp_alphas[ccp_alphas>=0])
# 
# clfs = []
# 
# for ccp_alpha in ccp_alphas:
#     clf = DecisionTreeClassifier(random_state=2360873, ccp_alpha=ccp_alpha).fit(pc_train, y_train)
#     clfs.append(clf)

train_scores = [clf.score(pc_train, y_train) for clf in clfs]
valid_scores = [clf.score(pc_valid, y_valid) for clf in clfs]

fig, ax = plt.subplots()
ax.set_xlabel("alpha")
ax.set_ylabel("Acurácia")
ax.set_title("Acurácia x alpha do conjunto de dados de treino e teste")
nx=-10
ax.plot(ccp_alphas[:nx], train_scores[:nx], marker='o', label="treino",    drawstyle="steps-post")
ax.plot(ccp_alphas[:nx], valid_scores[:nx], marker='o', label="validação", drawstyle="steps-post")

ax.legend()
plt.show()

ind_melhor_arvore = len(valid_scores) - valid_scores[::-1].index(max(valid_scores)) - 1
melhor_arvore = clfs[ind_melhor_arvore]

print(f'Acurácia da melhor árvore na base de treino: {train_scores[ind_melhor_arvore]*100:.1f}')
print(f'Acurácia da melhor árvore na base de validação: {valid_scores[ind_melhor_arvore]*100:.1f}')
print(f'Acurácia da melhor árvore na base de teste: {melhor_arvore.score(pc_test, y_test)*100:.1f}')

melhor_arvore