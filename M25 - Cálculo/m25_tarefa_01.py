# -*- coding: utf-8 -*-
"""m25_tarefa_01.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nL5r5hx5uGY4DvKhWPnKZkF9XB358iaX

# Cálculo - Tarefa 01

## 1. Marque quais desses métodos/algoritmos muito populares em ciência de dados são baseados no uso de derivada:

1. Método Mínimos Quadrados: SIM → a solução clássica da regressão linear é obtida derivando a função de erro quadrático e igualando as derivadas (ou gradiente) a zero. Usa derivadas explicitamente na dedução.

    Tecnologias/Ferramentas:
    - scikit-learn (LinearRegression);
    - statsmodels (OLS);
    - Numpy/SciPy (resolução via álgebra linear);
    - R (lm()).

    Características técnicas:
    - Solução analítica (equações normais);
    - Baixo custo computacional para bases pequenas e médias;
    - Muito usado em modelagem estatística, análise exploratória e inferência.

2. Gradiente descendente: SIM → o método se baseia no gradiente da função de custo, isto é: derivadas parciais em relação aos parâmetros. A direção de atualização vem diretamente das derivadas.
    
    Tecnologias/Ferramentas:
    - scikit-learn (SGDRegressor, SGDClassifier);
    - TensorFlow;
    - PyTorch;
    - JAX.

    Características técnicas:
    - Algoritmo iterativo;
    - Escala bem para grandes volumes de dados;
    - Permite:
      - treinamento online;
      - paralelização.
    - Base de redes neurais e modelos deep learning.

3. Newton Raphson: SIM → método iterativo baseado em: primeira derivada (gradiente) e segunda derivada (Hessiana). Extremamente dependente de cálculo diferencial.

    Tecnologias/Ferramentas:
    - statsmodels;
    - SciPy (optimize);
    - Implementações internas em regressão logística.

    Características técnicas:
    - Convergência rápida (ordem quadrática);
    - Exige cálculo da Hessiana;
    - Alto custo computacional em alta dimensão;
    - Usado em:
      - regressão logística e clássica;
      - otimização estatística.

4. CART (Árvore de decisão): NÃO → usam critérios de divisão (Gini, entropia, MSE) e avaliam redução de impureza. O processo é combinatorial/discreto. Não envolve derivadas nem gradiente.

    Tecnologias/Ferramentas:
    - scikit-learn (DecisionTreeClassifier, DecisionTreeRegressor);
    - XGBoost;
    - LightGBM;
    - Spark MLlib;

    Características técnicas:
    - Algoritmo não paramétrico;
    - Não exige normalização;
    - Lida bem com:
      - não linearidades;
      - interações complexas.
    - Base para Random Forest e Boosting.

Características técnicas em um contexto de projeto ou sistema

| Algoritmo             | Tipo de otimização   | Escalabilidade | Diferenciável | Uso típico em projetos       |
| --------------------- | -------------------- | -------------- | ------------- | ---------------------------- |
| Mínimos Quadrados     | Analítica            | Baixa–média    | Sim           | Modelos lineares, inferência |
| Gradiente Descendente | Iterativa            | Alta           | Sim           | Big data, deep learning      |
| Newton–Raphson        | Iterativa (2ª ordem) | Baixa          | Sim           | Estatística clássica         |
| CART                  | Busca discreta       | Alta           | Não           | Sistemas explicáveis         |

## 2. Dada uma base de dados com uma variável resposta $y$ e um conjunto de variáveis explicativas. Considere uma estrutura de um modelo de regressão. Explique com suas palavras por que não é possível obter parâmetros que forneçam um erro quadrático médio (EQM) menor que o obtido com estimadores de mínimos quadrados.

Não é possível obter parâmetros com EQM menor do que os estimadores de mínimos quadrados orque esses estimadores são, por construção, o ponto global de mínimo da função de erro quadrático médio dentro da classe de modelos considerada.

Dado um modelo de regressão fixo, o erro quadrático médio é uma função dos parâmetros. O método dos mínimos quadrados encontra o valores desses parâmetros que minimizam globalmente essa função. Como se trata de uma função convexa, qualquer outro conjunto de parâmetros produzirá um erro quadrático médio maior ou igual, o que impossibilita obter um EQM menor dentro da mesma estrutura de modelo.
"""

# Gerando um conjunto de dados simples

import numpy as np
import matplotlib.pyplot as plt

np.random.seed(42)

n = 80
x = np.random.uniform(-2, 2, size=n)
y = 2 + 3 * x + np.random.normal(0, 0.8, size=n)

# Definindo o EQM como função de α e β

def mse(alpha, beta, x, y):
    y_hat = alpha + beta * x
    return np.mean((y - y_hat)**2)

# Criando uma grade de valores para α e β

alpha_vals = np.linspace(-1, 5, 200)
beta_vals = np.linspace(0, 6, 200)

A, B = np.meshgrid(alpha_vals, beta_vals)

# Calculando o EQM em toda a grade

MSE = np.zeros_like(A)

for i in range(A.shape[0]):
    for j in range(A.shape[1]):
        MSE[i, j] = mse(A[i, j], B[i, j], x, y)

# Encontrando o mínimo global (mínimos quadrados)

idx_min = np.unravel_index(np.argmin(MSE), MSE.shape)

alpha_star = A[idx_min]
beta_star = B[idx_min]

print(alpha_star, beta_star)

# Plotando o “fundo do vale” (gráfico de contorno)

plt.figure(figsize=(8, 6))

contours = plt.contour(A, B, MSE, levels=25)
plt.clabel(contours, inline=True, fontsize=8)

plt.scatter(alpha_star, beta_star, marker="x", s=120)
plt.xlabel("α (intercepto)")
plt.ylabel("β (inclinação)")
plt.title("Função de perda (EQM) – único mínimo global")

plt.show()

"""O gráfico da função de perda mostra que o erro quadrático médio é uma função convexa dos parâmetros do modelo. A existência de um único “fundo de vale” indica a presença de um mínimo global. O método dos mínimos quadrados encontra exatamente esse ponto, o que implica que qualquer outro conjunto de parâmetros resultará em um EQM maior ou igual."""