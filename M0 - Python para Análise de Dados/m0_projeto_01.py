# -*- coding: utf-8 -*-
"""m0_projeto_01.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zxPT4mpAQJWARThwq4BJyFv7uW0N92fe

<img src="https://raw.githubusercontent.com/andre-marcos-perez/ebac-course-utils/main/media/logo/newebac_logo_black_half.png" alt="ebac-logo">

---

# **Módulo** | Python: Projeto Final
Caderno de **Aula**<br>
Professor [André Perez](https://www.linkedin.com/in/andremarcosperez/)

---

# **Tópicos**

<ol type="1">
  <li>Introdução ao Kaggle;</li>
  <li>Exploração de dados;</li>
  <li>Transformação e limpeza de dados;</li>
  <li>Visualização de dados;</li>
  <li>Storytelling.</li>
</ol>

---

# **Aulas**

## 1\. Introdução ao Kaggle

[Kaggle](https://www.kaggle.com/) é a maior comunidade online de ciência de dados e aprendizado de máquina. A plataforma permite que usuários encontrem e publiquem conjuntos de **dados**, construam e compartilhem **notebooks** (como este do Google Colab) e participem de **competições** (que pagam muito dinheiro as vezes) e desafios de dados.

> Vamos publicar nosso notebook de exercícios na plataforma web do Kaggle para que você possa compartilhar tudo o que você aprendeu neste curso e compor o seu portfólio.

> Anotação #1:
>
> [Kaggle](https://www.kaggle.com/competitions) uma plataforma muito utilizada no meio dos dados, utilizada para compartilhar análises. Para aprender a respeito dessa plataforma, iremos realizar as três etapas de uma análise de dados:
>
> - Exploração de dados: a principal ferramenta utilizada para isso é o pandas, pois ela realiza a leitura de datasets realiza a análise inicial e ajuda na compreensão inicial na qual os dados estão;
>
> - Transformação e limpeza de dados: aqui nessa etapa realiza-se a normalização em que os dados estão armazenados, lidamos com dados faltantes e todas as demais etapas pra garantir que o conjunto de dados está limpo e de fato pronto para que possamos analisá-lo;
>
> - Visualização: com os dados consolidados e limpos para serem analisados, começam uma série de visualizações que ajudam a compreensão dos dados. Para isso é utilizado o pacote o pacote do seaborn (nós já o vimos em artigos anteriores).
>
> Ainda nessa aula veremos a respeito de storytelling: uma forma de unir todas essas três etapas para poder contar uma espécie de história, explicando todas as etapas e todos os insights que foram obtidos durante a análise dos dados.
>
> O Kaggle expor os notebooks do GoogleColab numa página da web e também é a maior comunidade online de ciência de dados e aprendizado de máquina. A plataforma permite que usuários encontrem e publiquem conjuntos de dados, construam e compartilhem notebooks e participem de competições (que pagam muito dinheiro às vezes) e desafios de dados.
>
> Essa página web fica num link que podemos compartilhar com quem quisermos e que pode compor nosso portfólio. Em seu menu lateral esquerdo, o Kaggle possui várias abas. Na aba “code”, é possível criar nossos próprios notebooks. Esses notebooks são similares aos notebooks do GoogleColab.

## 2\. Exploração de Dados

Vamos explorar dados de crédito presentes neste neste [link](https://raw.githubusercontent.com/andre-marcos-perez/ebac-course-utils/develop/dataset/credito.csv). Os dados estão no formato CSV e contém informações sobre clientes de uma instituição financeira. Em especial, estamos interessados em explicar a segunda coluna, chamada de **default**, que indica se um cliente é adimplente(`default = 0`), ou inadimplente (`default = 1`), ou seja, queremos entender o porque um cliente deixa de honrar com suas dívidas baseado no comportamento de outros atributos, como salário, escolaridade e movimentação financeira. Uma descrição completa dos atributos está abaixo.

> O atributo de interesse (`default`) é conhecido como **variável resposta** ou **variável dependente**, já os demais atributos que buscam explicá-la (`idade`, `salário`, etc.) são conhecidas como **variáveis explicatívas**, **variáveis independentes** ou até **variáveis preditoras**.

> Anotação #2:
>
> É aqui que teremos o primeiro contato com o conjunto de dados para construir o projeto. Iremos analisar os dados em três verticais diferentes:
>
> Lógica do negócio que está sendo representada pelo conjunto de dados. É nesse momento que se verifica a estrutura do arquivo que contém os dados, qual o formato do arquivo (se ele é um arquivo csv, um arquivo de texto, etc.), quantas linhas ele tem e quantas colunas. Por fim, daremos uma olhada na qualidade dos dados, tratando a respeito do que é schema, está relacionado com o tipo do dado armazenado e com o que ele representa e também falaremos a respeito de dados faltantes (isso representa se a base de dados é consistente ou não).
>
> Tomemos como exemplo a seguinte base de dados:

| Coluna  | Descrição |
| ------- | --------- |
| id      | Número da conta |
| default | Indica se o cliente é adimplente (0) ou inadimplente (1) |
| idade   | --- |
| sexo    | --- |
| depedentes | --- |
| escolaridade | --- |
| estado_civil | --- |
| salario_anual | Faixa do salario mensal multiplicado por 12 |
| tipo_cartao | Categoria do cartao: blue, silver, gold e platinium |
| meses_de_relacionamento | Quantidade de meses desde a abertura da conta |
| qtd_produtos | Quantidade de produtos contratados |
| iteracoes_12m | Quantidade de iteracoes com o cliente no último ano |
| meses_inatico_12m | Quantidade de meses que o cliente ficou inativo no último ano |
| limite_credito | Valor do limite do cartão de crédito |
| valor_transacoes_12m | Soma total do valor das transações no cartão de crédito no último ano |
| qtd_transacoes_12m | Quantidade total de transações no cartão de crédito no último ano |

Vamos começar lendos os dados num dataframe `pandas`.

> Anotação #3:
>
> Pandas é uma biblioteca para que o próprio Python fornece para extração de dados
"""

import pandas as pd

"""> Anotação #4:
>
> Em seguida iremos fazer a leitura do dataframe e o salvaremos numa variável chamada df:
"""

df = pd.read_csv('https://raw.githubusercontent.com/andre-marcos-perez/ebac-course-utils/develop/dataset/credito.csv', na_values='na')

"""> Anotação #5:
>
> E leremos as primeiras dez linhas do dataframe através do método head(). Há bastante explicações sobre esses métodos e outros métodos da biblioteca pandas nesse [link](https://www.brasildatascience.com/post/conhecendo-a-biblioteca-pandas):
"""

df.head(n=10)

"""Com o dados em mãos, vamos conhecer um pouco melhor a estrutura do nosso conjunto de dados.

### **2.1. Estrutura**
"""

df.shape # retorna uma tupla (qtd linhas, qtd colunas)

"""> Anotação #6:
>
> Mas e se utilizarmos esse método em cima da coluna de interesse (default), pra quando o cliente é adimplente ou inadimplente?
"""

df[df['default'] == 0].shape

df[df['default'] == 1].shape

qtd_total, _ = df.shape
qtd_adimplentes, _ = df[df['default'] == 0].shape
qtd_inadimplentes, _ = df[df['default'] == 1].shape

print(f"A proporcão clientes adimplentes é de {round(100 * qtd_adimplentes / qtd_total, 2)}%")
print(f"A proporcão clientes inadimplentes é de {round(100 * qtd_inadimplentes / qtd_total, 2)}%")

"""> Anotação #7:
>
> Isso significa que a base de dados em questão é desbalanceada, pois existem muito mais clientes adimplentes do que clientes inadimplentes nela. Isso em geral se aplica a muitas instituições financeiras. A maioria das instituições financeiras concedem crédito a pessoas que pagam e não concedem crédio àquelas pessoas que não pagam.
>
> Essas são as primeiras impressões do nosso conjunto de dados. Pudemos verificar a quantidade de linhas, a quantidade de colunas, proporcionalmente à variável resposta.

### **2.2. Schema**

> Anotação #8:
>
> Falando um pouco mais sobre o schema em si e a qualidade do dado em si, é importante destacar que o schema relaciona o assunto tratado por uma coluna com o tipo do dado que está presente ali. Exemplo:
"""

df.head(n=5)

"""- Colunas e seus respectivos tipos de dados.

> Anotação #9:
>
> Os valores da coluna idade sempre serão um número inteiro, pois não existem idades com valores decimais. Já os valores da coluna sexo serão uma string representando se a pessoa é do sexo masculino (’M’) ou feminino (’F’), enquanto os valores da coluna dependentes serão inteiros representando quantos dependentes aquela pessoa possui, e assim por diante.
>
> O pandas possui um método que nos ajuda a verificar isso:
"""

df.dtypes

"""- Atributos **categóricos**.

> Anotação #10:
>
> Porém, se observamos nosso dataframe, podemos ver que as colunas “limite_credito” e “valor_transacoes” estão no formato object. E isso está ocorrendo porque esses dados estão não estão sendo impressos como floats típicos do Python, e sim como floats típicos da escrita brasileira. No Brasil, escrevemos os números decimais com vírgula, não com ponto. Portanto, os números das colunas abaixo estão sendo impressos como strings, não como floats.
>
> Ainda dentro desse contexto, é importante falar a respeito de atributos categóritos (ou variáveis categóricas), que são aqueles que estão relacionados a categorias. Atributos categóricos são aqueles decorrentes de observações de variáveis categóricas, ou seja, que identificam um caso para cada categoria. Em nossa tabela, os atributos categóricos serão strings (ou objects).
>
> Por exemplo: escolaridade. Trata-se de um número limitado de opções para categorizar aquele dado. Isso é verdade para o sexo, para a escolaridade, para o estado civilpara o salario anual e para o tipo de cartão dos clientes, mas não é verdade para o limite de crédito e para o valor de transações. Isso significa que esses dois últimos valores deverão ser tratados mais tarde.
>
> No trecho de código abaixo estamos selecionando todos os dados que são do tipo objeto (ou string) e em seguida utilizando o método describe() permite visualizar algumas informações estatísticas importantes. Já o método transpose() troca as linhas pelas colunas, tornando os nossos dados um pouco menos visualmente poluídos para nós os estudarmos:
"""

df.select_dtypes('object').describe().transpose()

"""- Atributos **numéricos**.

> Anotação #11:
>
> Novamente, podemos ver (e ainda mais claramente) que os valores de limite de crédito e de valor de transação estão presentes nessa lista. Esse dois valores são quantitativos, não qualitativos, comparados com o resto da lista.
>
> Porém, outro ponto importante a ser observado é que a contabilidade das colunas de escolaridade, estado civil e salário anual não bate com o número de linhas do dataframe. Isso indica que pode haver alguma informação faltando nessas colunas. Isso representa algum outro fator que vai precisar ser tratado em outro momento.
>
> Em seguida, faremos o mesmo com os as variáveis quantitativas do dataframe, exceto pelo id, que é uma variável que não precisa entrar em análise, por se tratar apenas de um identificador:
"""

df.drop('id', axis=1).select_dtypes('number').describe().transpose()

"""> Anotação #12:
>
> É possível observar, pelo resultado acima, que as métricas que o pandas traz para as variáveis numéricas são diferentes das métricas que o pandas traz para as variáveis qualitativas. Porém, a mesma análisa que é feita com os atributos anteriores, é feita com esses atributos, pois existe a possibilidade de termos que fazer algum tipo de tratamento com eles.
>
> Olhando a relação da quantidade de linhas com a contabilidade das variávies, é possível ver que não há nenhum atributo vazio. Nossa variável resposta está consistente, então não precisamos fazer nenhum tipo de tratamento em especial com ela. Porém, ao observar as outras variáveis é possível tirar várias outras conclusões, com a quantidade média de transações por ano, a quantidade média de meses que um cliente fica inativo, etc.

### **2.3. Dados faltantes**

Dados faltantes podem ser:

 - Vazios (`""`);
 - Nulos (`None`);
 - Não disponíveis ou aplicaveis (`na`, `NA`, etc.);
 - Não numérico (`nan`, `NaN`, `NAN`, etc).
"""

df.head()

"""Podemos verificar quais colunas possuem dados faltantes.

> Anotação #13:
>
> É importante, porém, verificar no dataframe qual desses tipos de dados está presente nele e se está. Faremos isso no trecho de código abaixo. O método isna() do pandas ajuda a detectar possíveis valores faltantes, enquanto o método any() retorna True se o item correspondente for verdadeiro ou False, caso contrário:
"""

df.isna().any()

"""> Anotação #14:
>
> Como é possível ver pela saída, apenas três variáveis possuem dados faltantes, a variável escolaridade, a variável estado_civil e a variável salario_anual. Através da função abaixo podemos ver qual a quantidade de dados faltantes e também a porcentagem relativa dela com a quantidade total de linha:

- A função abaixo levanta algumas estatisticas sobre as colunas dos dados faltantes.
"""

def stats_dados_faltantes(df: pd.DataFrame) -> None:

  stats_dados_faltantes = []
  for col in df.columns:
    if df[col].isna().any():
      qtd, _ = df[df[col].isna()].shape
      total, _ = df.shape
      dict_dados_faltantes = {col: {'quantidade': qtd, "porcentagem": round(100 * qtd/total, 2)}}
      stats_dados_faltantes.append(dict_dados_faltantes)

  for stat in stats_dados_faltantes:
    print(stat)

stats_dados_faltantes(df=df)

"""> Anotação #15:
>
> Podemos executar a função de uma maneira diferente, separando-a para um dos grupos que distinguimos no início do artigos: os adimplentes e os inadimplentes, de maneira que podemos trazer alguns insights a respeito do nosso dataframe:
"""

stats_dados_faltantes(df=df[df['default'] == 0])

stats_dados_faltantes(df=df[df['default'] == 1])

"""> Anotação #16:
>
> Ao fazer isso, obtemos mais segurança para eliminar essas linhas. Ao eliminar todas as linhas com dados faltantes, isso irá gerar um impacto igual nas duas categorias. Imagine que só houvesse dados faltantes no grupo de inadimplentes ou o contrário. Isso iria gerar algum de desbalanceio no dataframe.

## 3\. Transformação e limpeza de dados

Agora que conhecemos melhor a natureza do nosso conjunto de dados, vamos conduzir uma atividade conhecida como *data wrangling* que consiste na transformação e limpeza dos dados do conjunto para que possam ser melhor analisados. Em especial, vamos remover:

 - Corrigir o *schema* das nossas colunas;
 - Remover os dados faltantes.

### **3.1. Correção de schema**

Na etapa de exploração, notamos que as colunas **limite_credito** e **valor_transacoes_12m** estavam sendo interpretadas como colunas categóricas (`dtype = object`).
"""

df[['limite_credito', 'valor_transacoes_12m']].dtypes

df[['limite_credito', 'valor_transacoes_12m']].head(n=5)

"""Vamos criar uma função `lambda` para limpar os dados. Mas antes, vamos testar sua aplicação através do método funcional `map`:

> Anotação #17:
>
> A função lambda para limpar os dados utiliza o o método replace para substituir as vírgulas por pontos. Após isso as string são convertidas para floats. Mas antes, o método map vai iterar sobre uma lista dos quatro primeiros valores do limite_credito:
"""

fn = lambda valor: float(valor.replace(".", "").replace(",", "."))

valores_originais = ['12.691,51', '8.256,96', '3.418,56', '3.313,03', '4.716,22']
valores_limpos = list(map(fn, valores_originais))

print(valores_originais)
print(valores_limpos)

"""Com a função `lambda` de limpeza pronta, basta aplica-la nas colunas de interesse.

> Anotação #18:
>
> A função apply() é do próprio Python e permite fazer isso, ou seja, aplicar uma função qualquer a uma coluna:
"""

df['valor_transacoes_12m'] = df['valor_transacoes_12m'].apply(fn)
df['limite_credito'] = df['limite_credito'].apply(fn)

"""Vamos descrever novamente o *schema*:"""

df.dtypes

""" - Atributos **categóricos**."""

df.select_dtypes('object').describe().transpose()

""" - Atributos **numéricos**."""

df.drop('id', axis=1).select_dtypes('number').describe().transpose()

"""> Anotação #19:
>
> Podemos, então, observar que as colunas de interesse foram listadas como valores numéricos. O que significa que nosso processo de tratamento deu certo.

### **3.2. Remoção de dados faltantes**

Como o pandas está ciente do que é um dados faltante, a remoção das linhas problemáticas é trivial.
"""

df.dropna(inplace=True)

"""Vamos analisar a estrutura dos dados novamente."""

df.shape

df[df['default'] == 0].shape

df[df['default'] == 1].shape

qtd_total_novo, _ = df.shape
qtd_adimplentes_novo, _ = df[df['default'] == 0].shape
qtd_inadimplentes_novo, _ = df[df['default'] == 1].shape

print(f"A proporcão adimplentes ativos é de {round(100 * qtd_adimplentes / qtd_total, 2)}%")
print(f"A nova proporcão de clientes adimplentes é de {round(100 * qtd_adimplentes_novo / qtd_total_novo, 2)}%")
print("")
print(f"A proporcão clientes inadimplentes é de {round(100 * qtd_inadimplentes / qtd_total, 2)}%")
print(f"A nova proporcão de clientes inadimplentes é de {round(100 * qtd_inadimplentes_novo / qtd_total_novo, 2)}%")

"""> Anotação #20:
>
> Analisando as novas proporções  após o processo de limpeza, podemos ver que os valores são muito próximos um do outro. A base agora está pronta para podermos extrair insights a respeito de nossa variável resposta, o default.

## 4\. Visualização de dados

Os dados estão prontos, vamos criar diversas visualizações para correlacionar variáveis explicativas com a variável resposta para buscar entender qual fator leva um cliente a inadimplencia. E para isso, vamos sempre comparar a base com todos os clientes com a base de adimplentes e inadimplentes.

Começamos então importando os pacotes de visualização e separando os clientes adimplentes e inadimplentes

> Anotação #21
>
> A análise nada mais é do que a extração de insights a partir da visualização dos dados. Após explorar os dados, entender de que assunto ele está tratando, os problemas que eles trazem e tratar todos esses problemas na etapa de transformação e limpeza, os dados estão mais consistentes para que possamos correlacionar as variáveis independentes presentes no banco de dados com a variável dependente.
>
> Para isso, serão utilizados dois pacotes do Python muito utilizados para visualização de dados: o seaborn e o [matplotlib](https://matplotlib.org/). Este primeiro é, na verdade, um derivado do segundo. A princípio, estamos utilizando apenas algumas funcionalidades do matplotlib.
"""

import seaborn as sns
import matplotlib.pyplot as plt

sns.set_style("whitegrid")

df_adimplente = df[df['default'] == 0]

df_inadimplente = df[df['default'] == 1]

"""### **4.1. Visualizações categóricas**

> Anotação #22:
>
> Existem diversas maneiras de conduzir a análise de dados. Visualização é apenas uma delas.

Nesta seção, vamos visualizar a relação entre a variável resposta **default** com os atributos categóricos.
"""

df.select_dtypes('object').head(n=5)

"""> Anotação #23
>
> A primeira coluna que iremos comparar com a variável default é a coluna escolaridade:

- Escolaridade
"""

coluna = 'escolaridade'
titulos = ['Escolaridade dos Clientes', 'Escolaridade dos Clientes Adimplentes', 'Escolaridade dos Clientes Inadimplentes']

eixo = 0
max_y = 0
max = df.select_dtypes('object').describe()[coluna]['freq'] * 1.1

figura, eixos = plt.subplots(1,3, figsize=(20, 5), sharex=True)

for dataframe in [df, df_adimplente, df_inadimplente]:

  df_to_plot = dataframe[coluna].value_counts().to_frame()
  df_to_plot.rename(columns={coluna: 'frequencia_absoluta'}, inplace=True)
  df_to_plot[coluna] = df_to_plot.index
  df_to_plot.sort_values(by=[coluna], inplace=True)
  df_to_plot.sort_values(by=[coluna])

  f = sns.barplot(x=df_to_plot[coluna], y=df_to_plot['frequencia_absoluta'], ax=eixos[eixo])
  f.set(title=titulos[eixo], xlabel=coluna.capitalize(), ylabel='Frequência Absoluta')
  f.set_xticklabels(labels=f.get_xticklabels(), rotation=90)

  _, max_y_f = f.get_ylim()
  max_y = max_y_f if max_y_f > max_y else max_y
  f.set(ylim=(0, max_y))

  eixo += 1

figura.show()

"""> Anotação #24:
>
> Analisando os gráficos acima, podemos ver que os clientes do banco em geral possuem algum tipo de formação superior (primeiro gráfico). Porém, a quantidade de pessoas sem nenhum tipo de educação formal que são clientes do banco é grande. Ou seja, enquanto esse banco tem clientes bastante qualificados como clientes, ao mesmo tempo tem muitos clientes sem nenhum tipo de educação.
>
> Comparando a distribuição dos clientes totais com a distribuição dos clientes adimplentes, vemos que o padrão se mantém. Isso se dá pela quantidade de exemplos da base (mais de 80%).
>
> Em relação aos inadimplentes, a primeira informação que podemos tirar é que de fato temos poucos exemplos (apenas 16% da base). A segunda informação é que, mesmo que a frequência de clientes seja menor, a distribuição se mantém. O mesmo formato que tem de adimplentes, se aplica aos inadimplentes. A única diferença é em relação à barra de clientes com doutorado, que é um pouco maior que a barra de pessoas com graduação. No gráfico de clientes adimplentes, a barra de clientes com graduação era maior do que a barra de clientes com graduação.
>
> A principal conclusão que é possível tirar com essa análise é que olhar apenas para a escolaridade não nos ajuda a explicar o que torna um cliente adimplente ou inadimplente. É preciso fazer uma análise mais aprofundada.

- Salário Anual

> Anotação #24:
>
> A segunda coluna que iremos comparar com a variável default é a coluna de salário anual:
"""

coluna = 'salario_anual'
titulos = ['Salário Anual dos Clientes', 'Salário Anual dos Clientes Adimplentes', 'Salário Anual dos Clientes Inadimplentes']

eixo = 0
max_y = 0
figura, eixos = plt.subplots(1,3, figsize=(20, 5), sharex=True)

for dataframe in [df, df_adimplente, df_inadimplente]:

  df_to_plot = dataframe[coluna].value_counts().to_frame()
  df_to_plot.rename(columns={coluna: 'frequencia_absoluta'}, inplace=True)
  df_to_plot[coluna] = df_to_plot.index
  df_to_plot.reset_index(inplace=True, drop=True)
  df_to_plot.sort_values(by=[coluna], inplace=True)

  f = sns.barplot(x=df_to_plot[coluna], y=df_to_plot['frequencia_absoluta'], ax=eixos[eixo])
  f.set(title=titulos[eixo], xlabel=coluna.capitalize(), ylabel='Frequência Absoluta')
  f.set_xticklabels(labels=f.get_xticklabels(), rotation=90)
  _, max_y_f = f.get_ylim()
  max_y = max_y_f if max_y_f > max_y else max_y
  f.set(ylim=(0, max_y))
  eixo += 1

figura.show()

"""> Anotação #26:
>
> Pelo primeiro gráfico, é possível ver que a maioria dos clientes do banco ganha um salário anual de menos de 40K, enquanto o segundo valor mais comum é de 40K a 60K. O terceiro salário mais comum é de 60K a 120K, enquanto o quarto mais comum é de 60K a 80K e o menos comum de todos é de 120K por ano.
>
> Assim como aconteceu com a coluna de escolaridade, esse padrão se repetiu com o gráfico de clientes adimplentes. Porém, aqui, esse padrão também se repetiu da mesma maneira com os inadimplentes.
>
> Assim como a coluna de escolaridade, a coluna de salário anual não traz nenhum insight em particular a respeito da variável default.

### **4.2. Visualizações numéricas**

Nesta seção, vamos visualizar a relação entre a variável resposta **default** com os atributos numéricos.
"""

df.drop(['id', 'default'], axis=1).select_dtypes('number').head(n=5)

"""- Quantidade de Transações nos Últimos 12 Meses

> Anotação #27
>
> A primeira coluna numérica coluna que iremos comparar com a variável default é a coluna quantidade de transações nos últimos 12 meses:
"""

coluna = 'qtd_transacoes_12m'
titulos = ['Qtd. de Transações no Último Ano', 'Qtd. de Transações no Último Ano de Adimplentes', 'Qtd. de Transações no Último Ano de Inadimplentes']

eixo = 0
max_y = 0
figura, eixos = plt.subplots(1,3, figsize=(20, 5), sharex=True)

for dataframe in [df, df_adimplente, df_inadimplente]:

  f = sns.histplot(x=coluna, data=dataframe, stat='count', ax=eixos[eixo])
  f.set(title=titulos[eixo], xlabel=coluna.capitalize(), ylabel='Frequência Absoluta')

  _, max_y_f = f.get_ylim()
  max_y = max_y_f if max_y_f > max_y else max_y
  f.set(ylim=(0, max_y))

  eixo += 1

figura.show()

"""> Anotação #28
>
> Observando o primeiro gráfico, podemos ver que há dois picos: o primeiro representando 20 a 40 transações por ano que os clientes realizam e o segundo representando entre 60 a 80 transações por ano.  
>
> Partindo para o gráfico de clientes adimplentes, podemos ver que o pico de transações anuais realizadas por eles está concentrado no meio, entre 60, 80 e 100.
>
> Ao olhar para o gráfico de clientes inadimplentes, podemo ver que eles costumam realizar cerca de 20 a 60 transações por ano, gerando um pico nesse gráfico. Pico esse que no gráfico de clientes adimplentes, gera um vale e, que, quando olhamos para o gráfico de clientes totais, gera um pico ainda maior.
>
> Isso representa uma área em que clientes em geral se tornam indadimplentes, ou seja, que clientes, em geral, realizam o default. Essa é a conclusão geral que podemos tirar comparando essa coluna com a variável resposta.
>
> Além disso, essa observação representa também que esse banco deve acompanhar mais de perto os clientes que realizam 20 a 40 transações por ano, pois esses são os que têm mais probabilidade de se tornarem inadimplentes, de acordo com as observações feitas.

- Valor das Transações nos Últimos 12 Meses

> Anotação #29:
>
> A segunda coluna numérica que iremos comparar com a variável default é a coluna quantidade de valor das transações nos últimos 12 meses:
"""

coluna = 'valor_transacoes_12m'
titulos = ['Valor das Transações no Último Ano', 'Valor das Transações no Último Ano de Adimplentes', 'Valor das Transações no Último Ano de Inadimplentes']

eixo = 0
max_y = 0
figura, eixos = plt.subplots(1,3, figsize=(20, 5), sharex=True)

for dataframe in [df, df_adimplente, df_inadimplente]:

  f = sns.histplot(x=coluna, data=dataframe, stat='count', ax=eixos[eixo])
  f.set(title=titulos[eixo], xlabel=coluna.capitalize(), ylabel='Frequência Absoluta')

  _, max_y_f = f.get_ylim()
  max_y = max_y_f if max_y_f > max_y else max_y
  f.set(ylim=(0, max_y))

  eixo += 1

figura.show()

"""> Anotação #30:
>
> Observando a base total, vemos que o valor da transações está bastante concentrado entre 1.000 e 5.000 reais, havendo dois picos entre 7.000 e 10.000 e entre 12.500 e 17.500 reais.
>
> Já ao olhar para os clientes adimplentes, vemos que esse gráfico está bem representativo da base de clientes totais, inclusive os dois picos menores.
>
> Como no caso da coluna anterior, é possível ver que há um vale próximo ao valor de 2.500 reais, pois os clientes inadimplentes estão muito próximos dessa área. Portanto, podemos fazer uma correlação entre essa variável e a nossa variável default. Essa variável pode nos ajudar a traçar uma distinção ainda maior entre o que é ser adimplente e o que é ser inadimplente.
>
> Novamente, isso é um padrão de compartamento ao qual esse banco deve prestar mais atenção, pois existe uma maior probabilidade desses clientes se tornarem inadimplentes no futuro.
>
> Faremos, a seguir, uma comparação, entre essas duas variáveis.

- Valor de Transações nos Últimos 12 Meses x Quantidade de Transações nos Últimos 12 Meses
"""

f = sns.relplot(x='valor_transacoes_12m', y='qtd_transacoes_12m', data=df, hue='default')
_ = f.set(
    title='Relação entre Valor e Quantidade de Transações no Último Ano',
    xlabel='Valor das Transações no Último Ano',
    ylabel='Quantidade das Transações no Último Ano'
  )

"""> Anotação #31:
>
> No eixo x, temos o valor de transações no último ano, enquanto no y está a variável quantidade das transações no último ano.
>
> É perceptível que os clientes que realizaram transações de alto valor e em grande quantidade ao longo do ano ficaram agrupados no canto direito superior do gráfico e esses clientes são todos do grupo de adimplentes. Isso indica que essa instituição bancária não precisa se preocupar em especial com esse tipo de cliente, pois eles não correm risco de se tornarem default.
>
> Porém, conforme a quantidade de transações vai diminuindo e conforme o valor das transações também vai diminuindo, a chance dos clientes dessa instituição se tornarem default também aumenta.
>
> Dois grupos ficam mais claros ao olhar para o gráfico: um grupo concentrado em torno do valor de 7.500 e 10.000 reais e o outro grupo voltado para 1.000 e 6.000 reais. Porém, a quantidade de transações se torna mais espalhada para esses dois grupos, especialmente para o primeiro. O valor das transações parece explicar melhor o comportamento desses clientes.  
>
> Resumidamente, quanto menos transações e quanto menor o valor das transações realizadas por um cliente, maiores as chances dele se tornar inadimplente.

## 5\. Storytelling

O *storytelling* no contexto de dados é um técnica de apresentação de resultados orientado a dados, ou seja, contar uma história baseada nos *insights* que foram gerados através da análise dos dados. Notebooks como este do Google Colab e os do Kaggle são excelentes ferramentas para conduzir e compartilhar *storytelling* de dados devido a natureza texto-código de suas céluas.

> Anotação #32:
>
> O *storytelling* no contexto de dados é uma técnica de apresentação de resultados orientado a dados, ou seja, contar uma história baseada nos *insights* que foram gerados através da análise dos dados. Notebooks como do Google Colab e os do Kaggle são excelentes ferramentas para conduzir e compartilhar *storytelling* de dados devido a natureza texto-código de suas céluas.
>
> Ele pode utilizar de diversos recursos, como a escrita, o áudio, o vídeo, as animações e, até mesmo, os dados para encantar o interlocutor. Não é à toa que esse é um grande aliado do marketing para atrair um determinado público-alvo.
>
> Contar histórias para fazer com que dados sejam compreendidos de uma melhor maneira nunca foi tão necessário. Afinal, segundo o Fórum Econômico Mundial, até 2025, estima-se que serão gerados em todo o mundo, diariamente, 463 bilhões de gigabytes de dados.
>
> Como utilizar o data storytelling para uma apresentação atraente e que gere engajamento:
>
> - Definir o objetivo: o primeiro passo deve ser ter em mente qual é a mensagem principal que a apresentação deseja passar e que deve ficar na mente do público. O objetivo pode ser, por exemplo, emocionar, fazer refletir, despertar desejos, vender um produto ou serviço, entre tantos outros. A partir dessa definição, os dados podem ser inseridos com propósito, de uma forma que faça sentido;
> - Entender o público: esse é o ponto principal em qualquer tipo de comunicação. Identificar o público-alvo é essencial para saber o quanto é possível aprofundar nos dados apresentados, qual linguagem utilizar, que narrativa irá engajar mais, entre outros pontos. Uma apresentação feita para investidores é diferente daquela elaborada para funcionários de uma empresa, por exemplo;
> - Não ser prolixo: uma boa narrativa precisa ter início, meio e fim. A partir dessa estrutura podem ser inseridos elementos criativos, mas sem perder a objetividade. Quanto mais idas e vindas, maiores as chances do público perder a atenção;
> - Investir em recursos visuais: recursos como fotos, vídeos, gifs e gráficos interativos, por exemplo, são diferenciais para atrair o público e deixá-lo envolvido. No entanto, é importante que eles também não sejam usados de forma exagerada.

Para você montar o seu portifólio, eu sugiro a construção de um notebook com a seguinte estrutura (vou disponibiliza-la nos exercícios):

 1. Título;
 2. Breve descrição do problema;
 3. Código de importação de bibliotecas;
 4. Código o download/carregamento/geração de dados;
 5. Etapa de exploração;
 6. Etapa de limpeza e transformação;
 7. Etapa de análise (com visualizações);
 8. Resumo dos *insights* gerados.

Busquei organizar este notebook desta forma. Ademais, os notebooks presentes na plataforma do Kaggle são excelentes exemplos a serem seguidos, em especial os primeiros colocados em competições.

Para finalizar, algumas dicas:

 1. Estruture seu código sempre de acordo com as boas práticas PEP8, assim ele será mais legível para o leitor;
 2. Sempre se preocupe com a aparência dos seus gráficos, todos devem ter (no mínimo) título no topo e nos eixos;
 3. Use e abuso das células de texto para estruturar seu notebook, siga as mesmas técnicas que eu utilizo nos notebooks do curso para estruturar seu texto.

____

Quando terminar, se possível, entra em contato comigo, eu adoraria ver o seu notebook final pronto! Meu LinkedIn está no topo da página, logo após o logo da EBAC. Espero que você tenha gostado, foi um prazer trabalhar com você!

Até a próxima.
"""