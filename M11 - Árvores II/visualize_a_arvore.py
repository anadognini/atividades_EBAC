# -*- coding: utf-8 -*-
"""visualize_a_arvore.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bdzJ3BdeLWWvSi6eLNdUuMiMijwnzVU2
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn import tree
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# %matplotlib inline

tips = sns.load_dataset("tips")
tips['tip_pct'] = tips['tip'] / (tips['total_bill'] - tips['tip'])
tips.head()

# Construindo a árvore

# Variáveis explicativas

X = tips.drop(columns=['total_bill', 'tip', 'tip_pct']).copy()
X = pd.get_dummies(X, drop_first=True)

print(X.info())
X.head()

# Variável resposta

y = tips.loc[:, 'tip']
y.head()

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2360873)

regr_1 = DecisionTreeRegressor(max_depth=4, min_samples_leaf=5)
regr_1.fit(X_train, y_train)

mse1 = regr_1.score(X_train, y_train)
mse1

# Grid search -> uma busca pelo algoritmo ótimo ou pelos parâmetros delo

mses = []
ind_i = []
ind_j = []

for i in range(1, 9):
  for j in range(5, 30):
    regr_1 = DecisionTreeRegressor(max_depth=i, min_samples_leaf=j)
    regr_1.fit(X_train, y_train)
    mse1 = regr_1.score(X_test, y_test)
    mses.append(mse1)
    ind_i.append(i)
    ind_j.append(j)

df_mse = pd.DataFrame({'mses': mses, 'profundidade': ind_i, 'n_minimo': ind_j})
sns.heatmap(df_mse.pivot(index='profundidade', columns='n_minimo', values='mses'))

