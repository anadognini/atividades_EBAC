# -*- coding: utf-8 -*-
"""identifique_a_melhor_variavel_algoritmo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1h78rj_loNsbZd68RAejUvNGAvsGMyTv5
"""

# Lembrando que a principal medida de impureza da árvore de regressão é o
# Erro Quadrático Médio (EQM)

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn import tree
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# %matplotlib inline

tips = sns.load_dataset('tips')
tips['tip_pct'] = tips['tip'] / (tips['total_bill'] - tips['tip'])
tips.head()

"""### Construindo a árvore"""

# Variáveis explicativas

X = tips.drop(columns=['total_bill', 'tip', 'tip_pct']).copy()
X = pd.get_dummies(X, drop_first=True)

print(X.info())
X.head()

# Variável resposta

y = tips.loc[:, 'tip']
y.head()

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2360873)

# Estamos testando aqui duas árvores: uma com profundidade 2 e outra com
# profundidade 8

regr_1 = DecisionTreeRegressor(max_depth=2)
regr_1.fit(X_train, y_train)

regr_2 = DecisionTreeRegressor(max_depth=8)
regr_2.fit(X_train, y_train)

# Como ficou a qualidade da árvore?

mse1 = regr_1.score(X_train, y_train)
mse2 = regr_2.score(X_train, y_train)

template = "O R² da árvore com profundade={0} é {1:.2f}"
print(template.format(regr_1.get_depth(), mse1).replace(".", ","))
print(template.format(regr_2.get_depth(), mse2).replace(".", ","))