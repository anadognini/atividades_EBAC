# -*- coding: utf-8 -*-
"""realize_a_pos_poda.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17a4ydlgD9pKcEUn5_C3ccD0DKWxH_F4z
"""

import numpy as np
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn import tree
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# %matplotlib inline

tips = sns.load_dataset("tips")
tips['tip_pct'] = tips['tip'] / (tips['total_bill'] - tips['tip'])
tips.head()

# Construindo a árvore

# Variáveis explicativas

X = tips.drop(columns=['total_bill', 'tip', 'tip_pct']).copy()
X = pd.get_dummies(X, drop_first=True)

print(X.info())
X.head()

# Variável resposta

y = tips.loc[:, 'tip']
y.head()

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2360873)

regr_1 = DecisionTreeRegressor(max_depth=4, min_samples_leaf=5)
regr_1.fit(X_train, y_train)

regr_2 = DecisionTreeRegressor(max_depth=8)
regr_2.fit(X_train, y_train)

"""### Pós-poda"""

path = regr_2.cost_complexity_pruning_path(X_train, y_train)
path

# ccp_alphas = ccp
# impurities = MSE

# A árvore da árvore em função do Alpha

ccp_alphas, impurities = path.ccp_alphas, path.impurities

plt.figure(figsize=(10, 6))
plt.plot(ccp_alphas, impurities)
plt.xlabel("Alpha efetivo")
plt.ylabel("Impureza total das folhas")

# Profundidade da árvore em função de Alpha

clfs = []

for ccp_alpha in ccp_alphas:
  clf = DecisionTreeRegressor(random_state=0, ccp_alpha=ccp_alpha)
  clf.fit(X_train, y_train)
  clfs.append(clf)

tree_depths = [clf.tree_.max_depth for clf in clfs]
plt.figure(figsize=(10, 6))
plt.plot(ccp_alphas[:-1], tree_depths[:-1])
plt.xlabel("Alpha efetivo")
plt.ylabel("Profundidade da árvore")

# Conforme aumentamos o ccp, ele vai penalizando mais ramos da árvore e tornando
# as árvores mais enxutas

# MSE da árvore em função do alpha

train_scores = [mean_squared_error(y_train, clf.predict(X_train)) for clf in clfs]
test_scores = [mean_squared_error(y_test, clf.predict(X_test)) for clf in clfs]

fig, ax = plt.subplots()
ax.set_xlabel("Alpha efetivo")
ax.set_ylabel("MSE")
ax.set_title("MSE X Alpha do conjunto de dados de treino e teste")
ax.plot(ccp_alphas[:-1], train_scores[:-1], marker='o', label="treino",
        drawstyle="steps-post")
ax.plot(ccp_alphas[:-1], test_scores[:-1], marker='o', label="teste",
        drawstyle="steps-post")
ax.legend()
plt.show()

# Na base de testes -> overfitting

arvore_final = DecisionTreeRegressor(random_state=0, ccp_alpha=.01)
arvore_final.fit(X_train, y_train)

print(f"Profundidade: {arvore_final.tree_.max_depth}")
print(f"R² na base de testes: {arvore_final.score(X_test, y_test):.2f}")
print(f"MSE na base de testes: {mean_squared_error(y_test, arvore_final.predict(X_test)):.2f}")