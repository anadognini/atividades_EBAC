# -*- coding: utf-8 -*-
"""realize_boosting_no_python.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-5rQxYMO6bvTWIQYaHeXm7XDPQVOrx07
"""

import patsy
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from datetime import datetime
from scipy.stats import ks_2samp
from sklearn.metrics import accuracy_score, roc_auc_score
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingClassifier

df = pd.read_csv("credit_scoring.csv", parse_dates=['data_ref'])
df['tempo_emprego'].fillna(-1, inplace=True)
df.head()

# Dividindo em três datasets
# 1 validação - validação out-of-time, consiste em pegar o período mais recente do conjunto de dados (de 2016 pra cima)

df_val = df[df['data_ref'] >= datetime(2016, 1, 1)].copy()

# os outros dois: treino e teste
df = df[df['data_ref'] < datetime(2016, 1, 1)]

df_train, df_test = train_test_split(df, test_size=0.3, random_state=12)

df_train = df_train.reset_index(drop=True)
df_test = df_test.reset_index(drop=True)
df_val = df_val.reset_index(drop=True)

print(f'Quantidade de linhas no treino: {df_train.shape[0]}')
print(f'Quantidade de linhas no teste: {df_test.shape[0]}')
print(f'Quantidade de linhas na validação: {df_val.shape[0]}')

equacao = '''mau ~ sexo + posse_de_veiculo + posse_de_imovel + qtd_filhos +
           tipo_renda + educacao + estado_civil + tipo_residencia + idade
           + qt_pessoas_residencia + renda'''
y_train, X_train = patsy.dmatrices(equacao, data=df_train)
y_test, X_test = patsy.dmatrices(equacao, data=df_test)
y_val, X_val = patsy.dmatrices(equacao, data=df_val)

def calcular_gini(resp, prob_default):
  # AUC
  auc = roc_auc_score(resp, prob_default)
  # Gini
  gini = 2 * auc - 1

  return gini

def print_metricas(dados, prob_default='PD', classe_predita='classe_predita', resp='mau'):
  # Acurácia
  acc = accuracy_score(dados[resp], dados[classe_predita])
  # AUC
  auc = roc_auc_score(dados[resp], dados[prob_default])
  # Gini
  gini = 2 * auc -1
  # KS
  ks = ks_2samp(dados.loc[dados[resp] == 1, prob_default],
                dados.loc[dados[resp] != 1, prob_default]).statistic

  print(f'Acurácia: {acc * 100:.2f}%')
  print(f'AUC: {auc * 100:.2f}%')
  print(f'Gini: {gini * 100:.2f}%')
  print(f'KS: {ks * 100:.2f}%')

  return None

"""## Gradient Boosting Machine (GBM)"""

parametros = {
    'n_estimators': 100,
    'max_depth': None,
    'min_samples_leaf': 5,
    'learning_rate': 0.1,
    'random_state': 22
}

clf = GradientBoostingClassifier(**parametros).fit(X_train, y_train.ravel())

"""### Desempenho do GBM"""

df_train['classe_predita'] = clf.predict(X_train)
df_train['PD'] = clf.predict_proba(X_train)[:, 1]

df_test['classe_predita'] = clf.predict(X_test)
df_test['PD'] = clf.predict_proba(X_test)[:, 1]

df_val['classe_predita'] = clf.predict(X_val)
df_val['PD'] = clf.predict_proba(X_val)[:, 1]

print('Performance do GBM nos dados de treino')
print_metricas(dados=df_train)
print('Performance do GBM nos dados de teste')
print_metricas(dados=df_test)
print('Performance do GBM nos dados de validação')
print_metricas(dados=df_val)

"""### GridSearchCV 1"""

from sklearn.model_selection import GridSearchCV

gb = GradientBoostingClassifier()

parametros = {
    'n_estimators': [100, 300, 600],
    'min_samples_leaf': [2, 10, 20],
    'learning_rate': [0.04, 0.06, 0.1]
}

grid = GridSearchCV(estimator=gb, param_grid=parametros, scoring='roc_auc',
                    verbose=False, cv=2)

grid.fit(X_train, y_train.ravel())

grid.best_params_

"""### GridSearchCV 2"""

parametros = {
    'n_estimators': [90, 100, 110],
    'min_samples_leaf': [8, 10, 12],
    'learning_rate': [0.02, 0.03, 0.04]
}

grid = GridSearchCV(estimator=gb, param_grid=parametros, scoring='roc_auc',
                    verbose=False, cv=2)

grid.fit(X_train, y_train.ravel())

grid.best_params_

"""### GridSearchCV 3"""

parametros = {
    'n_estimators': [50, 100, 200],
    'min_samples_leaf': [6, 7, 8],
    'learning_rate': [0.01, 0.015, 0.02]
}

grid = GridSearchCV(estimator=gb, param_grid=parametros, scoring='roc_auc',
                    verbose=False, cv=2)

grid.fit(X_train, y_train.ravel())

grid.best_params_

"""### GridSearchCV 4"""

parametros = {
    'n_estimators': [100],
    'min_samples_leaf': [8],
    'learning_rate': [0.008, 0.009, 0.01]
}

grid = GridSearchCV(estimator=gb, param_grid=parametros, scoring='roc_auc',
                    verbose=False, cv=2)

grid.fit(X_train, y_train.ravel())

grid.best_params_

"""### Treinando e calculando a performance do modelo com os melhores hiperparãmetros"""

parametros = {
    'n_estimators': 100,
    'min_samples_leaf': 8,
    'learning_rate': 0.009
}

clf = GradientBoostingClassifier(**parametros).fit(X_train, y_train.ravel())

df_train['classe_predita'] = clf.predict(X_train)
df_train['PD'] = clf.predict_proba(X_train)[:, 1]

df_test['classe_predita'] = clf.predict(X_test)
df_test['PD'] = clf.predict_proba(X_test)[:, 1]

df_val['classe_predita'] = clf.predict(X_val)
df_val['PD'] = clf.predict_proba(X_val)[:, 1]

print('Performance do GBM nos dados de treino')
print_metricas(dados=df_train)
print('Performance do GBM nos dados de teste')
print_metricas(dados=df_test)
print('Performance do GBM nos dados de validação')
print_metricas(dados=df_val)